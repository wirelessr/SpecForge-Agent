"""
LLM Document Consistency Integration Tests.

This module tests cross-document consistency validation functionality with real LLM
interactions. It validates that documents generated by different agents maintain
alignment and coherence throughout the workflow progression.

Test Categories:
1. Requirements-Design Alignment - Tests alignment between requirements.md and design.md
2. Design-Tasks Alignment - Tests alignment between design.md and tasks.md  
3. Cross-Document Revision Consistency - Tests consistency across document revisions
4. Requirement Traceability - Tests requirement references through all documents
5. Coherent Workflow Progression - Tests logical progression through workflow phases

Requirements Covered:
- 4.4: Cross-document consistency validation between requirements, design, and tasks
- 5.4: Consistency across document revisions and iterative improvement
"""

import pytest
import asyncio
import json
import re
import tempfile
from pathlib import Path
from typing import Dict, Any, List, Tuple, Optional
from dataclasses import dataclass

from tests.integration.test_llm_base import (
    LLMIntegrationTestBase, 
    sequential_test_execution,
    QUALITY_THRESHOLDS_LENIENT
)
from autogen_framework.agents.plan_agent import PlanAgent
from autogen_framework.agents.design_agent import DesignAgent
from autogen_framework.agents.tasks_agent import TasksAgent
from autogen_framework.memory_manager import MemoryManager
from autogen_framework.context_manager import ContextManager
from autogen_framework.models import LLMConfig


@dataclass
class DocumentSet:
    """Set of related documents for consistency testing."""
    requirements_content: str
    design_content: str
    tasks_content: str
    user_request: str
    project_name: str


@dataclass
class ConsistencyTestScenario:
    """Test scenario for document consistency validation."""
    name: str
    description: str
    user_request: str
    expected_consistency_elements: List[str]  # Elements that should be consistent
    validation_criteria: Dict[str, Any]


class DocumentConsistencyValidator:
    """
    Helper class for validating consistency between documents.
    
    This class provides methods to analyze alignment between requirements,
    design, and tasks documents using both structural and semantic analysis.
    """
    
    def __init__(self, quality_validator):
        """
        Initialize document consistency validator.
        
        Args:
            quality_validator: LLMQualityValidator instance for document analysis
        """
        self.quality_validator = quality_validator
    
    def validate_requirements_design_alignment(self, requirements: str, design: str) -> Dict[str, Any]:
        """
        Validate alignment between requirements and design documents.
        
        Args:
            requirements: Requirements document content
            design: Design document content
            
        Returns:
            Dictionary containing alignment validation results
        """
        # Extract requirements from requirements document
        req_analysis = self._extract_requirements_elements(requirements)
        
        # Extract design elements from design document
        design_analysis = self._extract_design_elements(design)
        
        # Check alignment
        alignment_result = {
            'requirements_covered': 0,
            'total_requirements': len(req_analysis['user_stories']),
            'uncovered_requirements': [],
            'design_sections_present': design_analysis['sections_present'],
            'architecture_addresses_requirements': False,
            'alignment_score': 0.0,
            'alignment_issues': [],
            'alignment_strengths': []
        }
        
        # Check if each requirement is addressed in design
        for req_id, req_content in req_analysis['user_stories'].items():
            if self._requirement_addressed_in_design(req_content, design):
                alignment_result['requirements_covered'] += 1
                alignment_result['alignment_strengths'].append(
                    f"Requirement {req_id} addressed in design"
                )
            else:
                alignment_result['uncovered_requirements'].append(req_id)
                alignment_result['alignment_issues'].append(
                    f"Requirement {req_id} not clearly addressed in design"
                )
        
        # Check if design has required sections (more flexible matching)
        required_sections = ['overview', 'architecture', 'components', 'data models']
        missing_sections = []
        for section in required_sections:
            # More flexible matching - look for partial matches
            section_found = False
            for design_section in design_analysis['sections_present']:
                if (section.lower() in design_section.lower() or 
                    (section == 'architecture' and 'architectural' in design_section.lower()) or
                    (section == 'data models' and 'data model' in design_section.lower()) or
                    (section == 'components' and 'component' in design_section.lower())):
                    section_found = True
                    break
            
            if not section_found:
                missing_sections.append(section)
                alignment_result['alignment_issues'].append(f"Missing design section: {section}")
        
        # Calculate alignment score
        if alignment_result['total_requirements'] > 0:
            coverage_score = alignment_result['requirements_covered'] / alignment_result['total_requirements']
            section_score = (len(required_sections) - len(missing_sections)) / len(required_sections)
            alignment_result['alignment_score'] = (coverage_score + section_score) / 2
        
        alignment_result['architecture_addresses_requirements'] = alignment_result['alignment_score'] > 0.7
        
        return alignment_result
    
    def validate_design_tasks_alignment(self, design: str, tasks: str) -> Dict[str, Any]:
        """
        Validate alignment between design and tasks documents.
        
        Args:
            design: Design document content
            tasks: Tasks document content
            
        Returns:
            Dictionary containing alignment validation results
        """
        # Extract design components
        design_analysis = self._extract_design_elements(design)
        
        # Extract task elements
        tasks_analysis = self._extract_task_elements(tasks)
        
        # Check alignment
        alignment_result = {
            'design_components_implemented': 0,
            'total_design_components': len(design_analysis['components']),
            'unimplemented_components': [],
            'tasks_reference_design': False,
            'implementation_coverage': 0.0,
            'alignment_score': 0.0,
            'alignment_issues': [],
            'alignment_strengths': []
        }
        
        # Check if each design component has corresponding tasks
        for component in design_analysis['components']:
            if self._component_has_implementation_tasks(component, tasks):
                alignment_result['design_components_implemented'] += 1
                alignment_result['alignment_strengths'].append(
                    f"Component '{component}' has implementation tasks"
                )
            else:
                alignment_result['unimplemented_components'].append(component)
                alignment_result['alignment_issues'].append(
                    f"Component '{component}' lacks implementation tasks"
                )
        
        # Check if tasks reference design elements
        alignment_result['tasks_reference_design'] = self._tasks_reference_design(tasks, design_analysis)
        
        # Calculate alignment scores
        if alignment_result['total_design_components'] > 0:
            alignment_result['implementation_coverage'] = (
                alignment_result['design_components_implemented'] / 
                alignment_result['total_design_components']
            )
        
        # Overall alignment score
        coverage_weight = 0.7
        reference_weight = 0.3
        alignment_result['alignment_score'] = (
            coverage_weight * alignment_result['implementation_coverage'] +
            reference_weight * (1.0 if alignment_result['tasks_reference_design'] else 0.0)
        )
        
        return alignment_result
    
    def validate_requirement_traceability(self, requirements: str, design: str, tasks: str) -> Dict[str, Any]:
        """
        Validate requirement traceability through all documents.
        
        Args:
            requirements: Requirements document content
            design: Design document content
            tasks: Tasks document content
            
        Returns:
            Dictionary containing traceability validation results
        """
        # Extract requirements
        req_analysis = self._extract_requirements_elements(requirements)
        
        # Extract task requirement references
        tasks_analysis = self._extract_task_elements(tasks)
        
        traceability_result = {
            'requirements_traced': 0,
            'total_requirements': len(req_analysis['user_stories']),
            'untraced_requirements': [],
            'invalid_references': [],
            'traceability_score': 0.0,
            'traceability_issues': [],
            'traceability_strengths': []
        }
        
        # Check if each requirement is traced through to tasks
        for req_id, req_content in req_analysis['user_stories'].items():
            # Check if requirement is addressed in design
            in_design = self._requirement_addressed_in_design(req_content, design)
            
            # Check if requirement is referenced in tasks
            in_tasks = self._requirement_referenced_in_tasks(req_id, tasks)
            
            if in_design and in_tasks:
                traceability_result['requirements_traced'] += 1
                traceability_result['traceability_strengths'].append(
                    f"Requirement {req_id} traced through design to tasks"
                )
            else:
                traceability_result['untraced_requirements'].append(req_id)
                missing_in = []
                if not in_design:
                    missing_in.append("design")
                if not in_tasks:
                    missing_in.append("tasks")
                traceability_result['traceability_issues'].append(
                    f"Requirement {req_id} not traced in: {', '.join(missing_in)}"
                )
        
        # Check for invalid task references
        for task_ref in tasks_analysis['requirement_references']:
            if not self._is_valid_requirement_reference(task_ref, req_analysis):
                traceability_result['invalid_references'].append(task_ref)
                traceability_result['traceability_issues'].append(
                    f"Invalid requirement reference in tasks: {task_ref}"
                )
        
        # Calculate traceability score
        if traceability_result['total_requirements'] > 0:
            traceability_result['traceability_score'] = (
                traceability_result['requirements_traced'] / 
                traceability_result['total_requirements']
            )
        
        return traceability_result
    
    def validate_revision_consistency(self, original_docs: DocumentSet, 
                                    revised_docs: DocumentSet, 
                                    feedback: str) -> Dict[str, Any]:
        """
        Validate consistency across document revisions.
        
        Args:
            original_docs: Original document set
            revised_docs: Revised document set
            feedback: Feedback that was provided for revision
            
        Returns:
            Dictionary containing revision consistency validation results
        """
        consistency_result = {
            'documents_improved': [],
            'documents_degraded': [],
            'consistency_maintained': True,
            'revision_quality_score': 0.0,
            'consistency_issues': [],
            'consistency_strengths': []
        }
        
        # Validate each document revision
        doc_types = [
            ('requirements', original_docs.requirements_content, revised_docs.requirements_content),
            ('design', original_docs.design_content, revised_docs.design_content),
            ('tasks', original_docs.tasks_content, revised_docs.tasks_content)
        ]
        
        improvement_scores = []
        
        for doc_type, original, revised in doc_types:
            if original != revised:  # Document was actually revised
                # Check if revision improved quality
                revision_assessment = self.quality_validator.validate_revision_improvement(
                    original, revised, feedback
                )
                
                if revision_assessment['shows_improvement']:
                    consistency_result['documents_improved'].append(doc_type)
                    consistency_result['consistency_strengths'].append(
                        f"{doc_type.title()} document improved after revision"
                    )
                    improvement_scores.append(revision_assessment['improvement_score'])
                else:
                    consistency_result['documents_degraded'].append(doc_type)
                    consistency_result['consistency_issues'].append(
                        f"{doc_type.title()} document quality degraded after revision"
                    )
                    improvement_scores.append(0.0)
        
        # Check if revised documents maintain consistency with each other
        revised_alignment = self.validate_cross_document_consistency(revised_docs)
        
        if revised_alignment['overall_consistency_score'] < 0.7:
            consistency_result['consistency_maintained'] = False
            consistency_result['consistency_issues'].append(
                "Cross-document consistency degraded after revision"
            )
        else:
            consistency_result['consistency_strengths'].append(
                "Cross-document consistency maintained after revision"
            )
        
        # Calculate overall revision quality score
        if improvement_scores:
            consistency_result['revision_quality_score'] = sum(improvement_scores) / len(improvement_scores)
        
        return consistency_result
    
    def validate_cross_document_consistency(self, docs: DocumentSet) -> Dict[str, Any]:
        """
        Validate overall consistency across all documents.
        
        Args:
            docs: Document set to validate
            
        Returns:
            Dictionary containing overall consistency validation results
        """
        # Validate pairwise alignments
        req_design_alignment = self.validate_requirements_design_alignment(
            docs.requirements_content, docs.design_content
        )
        
        design_tasks_alignment = self.validate_design_tasks_alignment(
            docs.design_content, docs.tasks_content
        )
        
        traceability = self.validate_requirement_traceability(
            docs.requirements_content, docs.design_content, docs.tasks_content
        )
        
        # Calculate overall consistency
        consistency_result = {
            'req_design_alignment': req_design_alignment,
            'design_tasks_alignment': design_tasks_alignment,
            'requirement_traceability': traceability,
            'overall_consistency_score': 0.0,
            'consistency_issues': [],
            'consistency_strengths': [],
            'workflow_coherence': True
        }
        
        # Aggregate scores
        alignment_scores = [
            req_design_alignment['alignment_score'],
            design_tasks_alignment['alignment_score'],
            traceability['traceability_score']
        ]
        
        consistency_result['overall_consistency_score'] = sum(alignment_scores) / len(alignment_scores)
        
        # Aggregate issues and strengths
        for alignment in [req_design_alignment, design_tasks_alignment, traceability]:
            consistency_result['consistency_issues'].extend(alignment.get('alignment_issues', []))
            consistency_result['consistency_issues'].extend(alignment.get('traceability_issues', []))
            consistency_result['consistency_strengths'].extend(alignment.get('alignment_strengths', []))
            consistency_result['consistency_strengths'].extend(alignment.get('traceability_strengths', []))
        
        # Check workflow coherence
        if consistency_result['overall_consistency_score'] < 0.5:
            consistency_result['workflow_coherence'] = False
            consistency_result['consistency_issues'].append(
                "Overall workflow coherence below acceptable threshold"
            )
        
        return consistency_result
    
    def _extract_requirements_elements(self, requirements: str) -> Dict[str, Any]:
        """Extract structured elements from requirements document."""
        elements = {
            'user_stories': {},
            'acceptance_criteria': {},
            'requirements_count': 0
        }
        
        # Handle code block wrapping - extract content between ``` markers
        content = requirements
        if '```' in requirements:
            # Find content between code blocks
            parts = requirements.split('```')
            if len(parts) >= 3:
                # Take the content between the first pair of ```
                content = parts[1]
            elif len(parts) == 2:
                # Content might be after the first ```
                content = parts[1]
        

        
        # Find user stories and requirements
        lines = content.split('\n')
        current_requirement = None
        
        for i, line in enumerate(lines):
            # Look for requirement headers (### Requirement N or ### Requirement N: Title)
            req_match = re.search(r'###\s+Requirement\s+(\d+)', line, re.IGNORECASE)
            if req_match:
                current_requirement = req_match.group(1)
                elements['requirements_count'] += 1
                continue
            
            # Look for user stories
            if current_requirement and 'user story' in line.lower():
                # Check if the user story is on the same line (after "User Story:")
                if ':' in line:
                    story_part = line.split(':', 1)[1].strip()
                    if story_part:
                        elements['user_stories'][current_requirement] = story_part
                # Also check the next line in case it's on a separate line
                elif i + 1 < len(lines):
                    story_line = lines[i + 1].strip()
                    if story_line:
                        elements['user_stories'][current_requirement] = story_line
            
            # Look for acceptance criteria
            if current_requirement and ('acceptance criteria' in line.lower() or 
                                      line.strip().startswith(('WHEN', 'IF', 'GIVEN'))):
                if current_requirement not in elements['acceptance_criteria']:
                    elements['acceptance_criteria'][current_requirement] = []
                
                # Collect EARS format criteria
                for j in range(i, min(i + 10, len(lines))):
                    criteria_line = lines[j].strip()
                    if criteria_line and re.match(r'^\d+\.\s+(WHEN|IF|GIVEN)', criteria_line):
                        elements['acceptance_criteria'][current_requirement].append(criteria_line)
        
        return elements
    
    def _extract_design_elements(self, design: str) -> Dict[str, Any]:
        """Extract structured elements from design document."""
        elements = {
            'sections_present': [],
            'components': [],
            'architecture_elements': [],
            'mermaid_diagrams': []
        }
        
        lines = design.split('\n')
        
        for line in lines:
            # Find section headers
            if line.startswith('#'):
                section_name = line.strip('#').strip()
                elements['sections_present'].append(section_name)
            
            # Find component mentions
            if 'component' in line.lower() or 'class' in line.lower():
                # Extract component names (simple heuristic)
                words = line.split()
                for word in words:
                    if word.endswith('Component') or word.endswith('Agent') or word.endswith('Manager'):
                        elements['components'].append(word)
            
            # Find Mermaid diagrams
            if line.strip().startswith('```mermaid'):
                elements['mermaid_diagrams'].append('mermaid_diagram')
        
        return elements
    
    def _extract_task_elements(self, tasks: str) -> Dict[str, Any]:
        """Extract structured elements from tasks document."""
        elements = {
            'tasks': [],
            'requirement_references': [],
            'task_count': 0
        }
        
        lines = tasks.split('\n')
        
        for line in lines:
            # Find task items (- [ ] or - [x])
            if re.match(r'^\s*-\s*\[[ x]\]', line):
                elements['tasks'].append(line.strip())
                elements['task_count'] += 1
            
            # Find requirement references (_Requirements: ...)
            req_ref_match = re.search(r'_Requirements?:\s*([^_]+)_', line)
            if req_ref_match:
                refs = req_ref_match.group(1).strip()
                # Split by comma and clean up
                for ref in refs.split(','):
                    ref = ref.strip()
                    if ref:
                        elements['requirement_references'].append(ref)
        
        return elements
    
    def _requirement_addressed_in_design(self, requirement: str, design: str) -> bool:
        """Check if a requirement is addressed in the design document."""
        # Simple keyword matching - could be enhanced with semantic analysis
        req_keywords = self._extract_keywords(requirement)
        design_lower = design.lower()
        
        # Check if key concepts from requirement appear in design
        matches = 0
        for keyword in req_keywords:
            if keyword.lower() in design_lower:
                matches += 1
        
        # Requirement is considered addressed if most keywords are present
        return matches >= len(req_keywords) * 0.5
    
    def _component_has_implementation_tasks(self, component: str, tasks: str) -> bool:
        """Check if a design component has corresponding implementation tasks."""
        tasks_lower = tasks.lower()
        component_lower = component.lower()
        
        # Look for component name or related terms in tasks
        return (component_lower in tasks_lower or 
                any(word in tasks_lower for word in component_lower.split()))
    
    def _tasks_reference_design(self, tasks: str, design_analysis: Dict[str, Any]) -> bool:
        """Check if tasks reference design elements."""
        tasks_lower = tasks.lower()
        
        # Check if any design components are mentioned in tasks
        for component in design_analysis['components']:
            if component.lower() in tasks_lower:
                return True
        
        # Check if design sections are referenced
        design_keywords = ['architecture', 'component', 'interface', 'model']
        return any(keyword in tasks_lower for keyword in design_keywords)
    
    def _requirement_referenced_in_tasks(self, req_id: str, tasks: str) -> bool:
        """Check if a requirement ID is referenced in tasks."""
        # Look for requirement references in format like "1.1", "2.3", etc.
        pattern = rf'\b{re.escape(req_id)}\b'
        return bool(re.search(pattern, tasks))
    
    def _is_valid_requirement_reference(self, task_ref: str, req_analysis: Dict[str, Any]) -> bool:
        """Check if a task requirement reference is valid."""
        # Check if the reference matches any known requirement
        return task_ref in req_analysis['user_stories']
    
    def _extract_keywords(self, text: str) -> List[str]:
        """Extract key terms from text for matching."""
        # Simple keyword extraction - remove common words
        stop_words = {'a', 'an', 'the', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by'}
        words = re.findall(r'\b\w+\b', text.lower())
        return [word for word in words if len(word) > 3 and word not in stop_words]


class TestLLMDocumentConsistency(LLMIntegrationTestBase):
    """
    Test class for LLM document consistency validation.
    
    This class tests cross-document consistency validation functionality with real LLM
    interactions, ensuring that documents generated by different agents maintain
    alignment and coherence throughout the workflow progression.
    """
    
    def setup_method(self, method):
        """Setup method called before each test."""
        super().setup_method(method)
        
        # Use lenient quality thresholds for consistency tests
        from tests.integration.test_llm_base import QUALITY_THRESHOLDS_LENIENT, LLMQualityValidator
        self.quality_validator = LLMQualityValidator(QUALITY_THRESHOLDS_LENIENT)
        self.consistency_validator = DocumentConsistencyValidator(self.quality_validator)
        
        # Test scenarios for consistency validation
        self.test_scenarios = [
            ConsistencyTestScenario(
                name="simple_web_app",
                description="Simple web application with user authentication",
                user_request="Create a simple web application with user registration and login functionality",
                expected_consistency_elements=[
                    "user registration", "login", "authentication",
                    "web application"
                ],
                validation_criteria={
                    'min_alignment_score': 0.4,  # More lenient for initial testing
                    'min_traceability_score': 0.5,  # More lenient for initial testing
                    'required_design_sections': ['overview', 'architecture', 'components'],
                    'min_task_count': 3  # More lenient for initial testing
                }
            ),
            ConsistencyTestScenario(
                name="data_processing_pipeline",
                description="Data processing pipeline with validation and reporting",
                user_request="Build a data processing pipeline that validates input data and generates reports",
                expected_consistency_elements=[
                    "data processing", "pipeline", "validation", "reports"
                ],
                validation_criteria={
                    'min_alignment_score': 0.4,  # More lenient for initial testing
                    'min_traceability_score': 0.5,  # More lenient for initial testing
                    'required_design_sections': ['overview', 'architecture', 'data models'],
                    'min_task_count': 3  # More lenient for initial testing
                }
            )
        ]
    
    @sequential_test_execution()
    @pytest.mark.integration
    async def test_requirements_design_alignment(self, initialized_real_managers, real_llm_config, real_memory_manager, temp_workspace):
        """
        Test alignment between requirements.md and design.md documents.
        
        This test validates that design documents properly address all requirements
        and maintain coherent architecture that supports the specified functionality.
        """
        managers = initialized_real_managers
        
        # Test with first scenario
        scenario = self.test_scenarios[0]
        
        # Generate requirements document using container
        from autogen_framework.dependency_container import DependencyContainer
        container = DependencyContainer.create_production(temp_workspace, real_llm_config)
        
        plan_agent = PlanAgent(
            container=container,
            name="PlanAgent",
            llm_config=real_llm_config,
            system_message="Generate project requirements"
        )
        
        # Parse the user request first
        parsed_request = await self.execute_with_rate_limit_handling(
            lambda: plan_agent.parse_user_request(scenario.user_request)
        )
        
        requirements_path = await self.execute_with_rate_limit_handling(
            lambda: plan_agent.generate_requirements(scenario.user_request, temp_workspace, parsed_request)
        )
        
        # Read the generated requirements content
        req_path = Path(requirements_path)
        assert req_path.exists(), f"Requirements file not created at {requirements_path}"
        
        requirements_content = req_path.read_text()
        

        
        # Validate requirements quality (use lenient thresholds for consistency tests)
        req_validation = self.quality_validator.validate_requirements_quality(requirements_content)
        # Log quality but don't fail on it - focus is on consistency, not absolute quality
        self.log_quality_assessment(req_validation)
        
        # Generate design document using container
        design_agent = DesignAgent(
            container=container,
            name="DesignAgent",
            llm_config=real_llm_config,
            system_message="Generate technical design documents"
        )
        
        design_content = await self.execute_with_rate_limit_handling(
            lambda: design_agent.generate_design(requirements_path, real_memory_manager.load_memory())
        )
        
        # Validate design quality (use lenient thresholds for consistency tests)
        design_validation = self.quality_validator.validate_design_quality(design_content)
        # Log quality but don't fail on it - focus is on consistency, not absolute quality
        self.log_quality_assessment(design_validation)
        
        # Validate requirements-design alignment
        alignment_result = self.consistency_validator.validate_requirements_design_alignment(
            requirements_content, design_content
        )
        

        
        # Log alignment assessment with detailed debugging
        self.logger.info(f"Requirements-Design Alignment Score: {alignment_result['alignment_score']:.2f}")
        self.log_quality_assessment({'assessment': alignment_result, 'output_type': 'alignment'})
        
        # Assert alignment quality
        assert alignment_result['alignment_score'] >= scenario.validation_criteria['min_alignment_score'], (
            f"Requirements-design alignment score {alignment_result['alignment_score']:.2f} "
            f"below threshold {scenario.validation_criteria['min_alignment_score']}"
        )
        
        assert alignment_result['architecture_addresses_requirements'], (
            "Design architecture does not adequately address requirements"
        )
        
        # Check that required design sections are present (use flexible matching)
        required_sections = scenario.validation_criteria['required_design_sections']
        missing_sections = []
        for section in required_sections:
            # Use the same flexible matching logic as in the validator
            section_found = False
            for design_section in alignment_result['design_sections_present']:
                if (section.lower() in design_section.lower() or 
                    (section == 'architecture' and 'architectural' in design_section.lower()) or
                    (section == 'data models' and 'data model' in design_section.lower()) or
                    (section == 'components' and 'component' in design_section.lower())):
                    section_found = True
                    break
            
            if not section_found:
                missing_sections.append(section)
        
        assert not missing_sections, f"Missing required design sections: {missing_sections}"
        
        # Validate coverage of requirements
        coverage_ratio = alignment_result['requirements_covered'] / max(1, alignment_result['total_requirements'])
        assert coverage_ratio >= 0.7, (
            f"Requirements coverage {coverage_ratio:.2f} below threshold 0.7. "
            f"Uncovered requirements: {alignment_result['uncovered_requirements']}"
        )
    
    @sequential_test_execution()
    @pytest.mark.integration
    async def test_design_tasks_alignment(self, initialized_real_managers, real_llm_config, real_memory_manager, temp_workspace):
        """
        Test alignment between design.md and tasks.md documents.
        
        This test validates that task lists properly implement all design components
        and maintain coherent implementation strategy.
        """
        managers = initialized_real_managers
        
        # Test with second scenario
        scenario = self.test_scenarios[1]
        
        # Generate complete document chain
        docs = await self._generate_document_chain(
            scenario.user_request, managers, real_llm_config, real_memory_manager, temp_workspace
        )
        
        # Validate design-tasks alignment
        alignment_result = self.consistency_validator.validate_design_tasks_alignment(
            docs.design_content, docs.tasks_content
        )
        
        # Log alignment assessment
        self.logger.info(f"Design-Tasks Alignment Score: {alignment_result['alignment_score']:.2f}")
        self.log_quality_assessment({'assessment': alignment_result, 'output_type': 'alignment'})
        
        # Assert alignment quality
        assert alignment_result['alignment_score'] >= scenario.validation_criteria['min_alignment_score'], (
            f"Design-tasks alignment score {alignment_result['alignment_score']:.2f} "
            f"below threshold {scenario.validation_criteria['min_alignment_score']}"
        )
        
        # Check implementation coverage
        assert alignment_result['implementation_coverage'] >= 0.7, (
            f"Implementation coverage {alignment_result['implementation_coverage']:.2f} below threshold 0.7. "
            f"Unimplemented components: {alignment_result['unimplemented_components']}"
        )
        
        # Validate that tasks reference design elements
        assert alignment_result['tasks_reference_design'], (
            "Tasks do not adequately reference design elements"
        )
        
        # Check minimum task count
        tasks_analysis = self.consistency_validator._extract_task_elements(docs.tasks_content)
        assert tasks_analysis['task_count'] >= scenario.validation_criteria['min_task_count'], (
            f"Task count {tasks_analysis['task_count']} below minimum "
            f"{scenario.validation_criteria['min_task_count']}"
        )
    
    @sequential_test_execution()
    @pytest.mark.integration
    async def test_requirement_traceability(self, initialized_real_managers, real_llm_config, real_memory_manager, temp_workspace):
        """
        Test requirement traceability through all documents.
        
        This test validates that requirements can be traced from requirements.md
        through design.md to tasks.md, ensuring complete workflow coherence.
        """
        managers = initialized_real_managers
        
        # Test with first scenario
        scenario = self.test_scenarios[0]
        
        # Generate complete document chain
        docs = await self._generate_document_chain(
            scenario.user_request, managers, real_llm_config, real_memory_manager, temp_workspace
        )
        
        # Validate requirement traceability
        traceability_result = self.consistency_validator.validate_requirement_traceability(
            docs.requirements_content, docs.design_content, docs.tasks_content
        )
        
        # Log traceability assessment
        self.logger.info(f"Requirement Traceability Score: {traceability_result['traceability_score']:.2f}")
        self.log_quality_assessment({'assessment': traceability_result, 'output_type': 'traceability'})
        
        # Assert traceability quality
        assert traceability_result['traceability_score'] >= scenario.validation_criteria['min_traceability_score'], (
            f"Requirement traceability score {traceability_result['traceability_score']:.2f} "
            f"below threshold {scenario.validation_criteria['min_traceability_score']}"
        )
        
        # Check that most requirements are traced
        trace_ratio = traceability_result['requirements_traced'] / max(1, traceability_result['total_requirements'])
        assert trace_ratio >= 0.8, (
            f"Requirement trace ratio {trace_ratio:.2f} below threshold 0.8. "
            f"Untraced requirements: {traceability_result['untraced_requirements']}"
        )
        
        # Validate that there are no invalid references
        assert not traceability_result['invalid_references'], (
            f"Invalid requirement references found: {traceability_result['invalid_references']}"
        )
    
    @pytest.mark.skip(reason="Revision test needs more work - skipping for now")
    @sequential_test_execution()
    @pytest.mark.integration
    async def test_cross_document_revision_consistency(self, initialized_real_managers, real_llm_config, real_memory_manager, temp_workspace):
        """
        Test consistency across document revisions.
        
        This test validates that when documents are revised based on feedback,
        they maintain consistency with each other and show meaningful improvement.
        """
        managers = initialized_real_managers
        
        # Test with second scenario
        scenario = self.test_scenarios[1]
        
        # Generate initial document set
        original_docs = await self._generate_document_chain(
            scenario.user_request, managers, real_llm_config, real_memory_manager, temp_workspace
        )
        
        # Provide feedback for revision
        feedback = (
            "Please add more detail about error handling in the design and "
            "ensure tasks include proper testing and validation steps"
        )
        
        # Generate revised documents
        revised_docs = await self._generate_revised_documents(
            original_docs, feedback, managers, real_llm_config, real_memory_manager, temp_workspace
        )
        
        # Validate revision consistency
        consistency_result = self.consistency_validator.validate_revision_consistency(
            original_docs, revised_docs, feedback
        )
        
        # Log consistency assessment
        self.logger.info(f"Revision Quality Score: {consistency_result['revision_quality_score']:.2f}")
        self.log_quality_assessment({'assessment': consistency_result, 'output_type': 'revision_consistency'})
        
        # Assert revision quality
        assert consistency_result['revision_quality_score'] >= 0.6, (
            f"Revision quality score {consistency_result['revision_quality_score']:.2f} below threshold 0.6"
        )
        
        # Check that consistency is maintained
        assert consistency_result['consistency_maintained'], (
            "Cross-document consistency not maintained after revision"
        )
        
        # Validate that at least some documents improved
        assert consistency_result['documents_improved'], (
            "No documents showed improvement after revision"
        )
        
        # Check that no documents degraded significantly
        assert not consistency_result['documents_degraded'], (
            f"Documents degraded after revision: {consistency_result['documents_degraded']}"
        )
    
    @sequential_test_execution()
    @pytest.mark.integration
    async def test_coherent_workflow_progression(self, initialized_real_managers, real_llm_config, real_memory_manager, temp_workspace):
        """
        Test coherent workflow progression through all documents.
        
        This test validates the overall coherence of the complete workflow
        from requirements through design to implementation tasks.
        """
        managers = initialized_real_managers
        
        # Test with first scenario only for now
        for scenario in [self.test_scenarios[0]]:
            self.logger.info(f"Testing workflow coherence for scenario: {scenario.name}")
            
            # Generate complete document chain
            docs = await self._generate_document_chain(
                scenario.user_request, managers, real_llm_config, real_memory_manager, temp_workspace
            )
            
            # Validate overall cross-document consistency
            consistency_result = self.consistency_validator.validate_cross_document_consistency(docs)
            
            # Log consistency assessment
            self.logger.info(f"Overall Consistency Score: {consistency_result['overall_consistency_score']:.2f}")
            self.logger.info(f"Workflow coherence: {consistency_result['workflow_coherence']}")
            self.logger.info(f"Consistency issues: {consistency_result['consistency_issues']}")
            self.log_quality_assessment({'assessment': consistency_result, 'output_type': 'workflow_coherence'})
            
            # Assert workflow coherence
            assert consistency_result['workflow_coherence'], (
                f"Workflow coherence failed for scenario {scenario.name}"
            )
            
            assert consistency_result['overall_consistency_score'] >= 0.5, (
                f"Overall consistency score {consistency_result['overall_consistency_score']:.2f} "
                f"below threshold 0.5 for scenario {scenario.name}"
            )
            
            # Validate individual alignment components
            req_design_score = consistency_result['req_design_alignment']['alignment_score']
            design_tasks_score = consistency_result['design_tasks_alignment']['alignment_score']
            traceability_score = consistency_result['requirement_traceability']['traceability_score']
            
            assert req_design_score >= 0.4, (
                f"Requirements-design alignment {req_design_score:.2f} below threshold 0.4"
            )
            
            assert design_tasks_score >= 0.3, (
                f"Design-tasks alignment {design_tasks_score:.2f} below threshold 0.3"
            )
            
            assert traceability_score >= 0.5, (
                f"Requirement traceability {traceability_score:.2f} below threshold 0.5"
            )
            
            # Validate expected consistency elements are present
            all_content = f"{docs.requirements_content} {docs.design_content} {docs.tasks_content}".lower()
            missing_elements = []
            
            for element in scenario.expected_consistency_elements:
                if element.lower() not in all_content:
                    missing_elements.append(element)
            
            assert not missing_elements, (
                f"Missing expected consistency elements in scenario {scenario.name}: {missing_elements}"
            )
    
    async def _generate_document_chain(self, user_request: str, managers, llm_config, memory_manager, workspace) -> DocumentSet:
        """
        Generate a complete chain of documents (requirements -> design -> tasks).
        
        Args:
            user_request: User request to process
            managers: Initialized managers
            llm_config: LLM configuration
            workspace: Temporary workspace
            
        Returns:
            DocumentSet containing all generated documents
        """
        # Generate requirements using container
        from autogen_framework.dependency_container import DependencyContainer
        container = DependencyContainer.create_production(workspace, llm_config)
        
        plan_agent = PlanAgent(
            container=container,
            name="PlanAgent",
            llm_config=llm_config,
            system_message="Generate project requirements"
        )
        
        # Parse the user request first
        parsed_request = await self.execute_with_rate_limit_handling(
            lambda: plan_agent.parse_user_request(user_request)
        )
        
        requirements_path = await self.execute_with_rate_limit_handling(
            lambda: plan_agent.generate_requirements(user_request, workspace, parsed_request)
        )
        
        # Read the generated requirements content
        req_path = Path(requirements_path)
        assert req_path.exists(), f"Requirements file not created at {requirements_path}"
        requirements_content = req_path.read_text()
        
        # Generate design using container
        design_agent = DesignAgent(
            container=container,
            name="DesignAgent",
            llm_config=llm_config,
            system_message="Generate technical design documents"
        )
        
        design_content = await self.execute_with_rate_limit_handling(
            lambda: design_agent.generate_design(requirements_path, memory_manager.load_memory())
        )
        
        # Generate tasks using container
        tasks_agent = TasksAgent(
            container=container,
            name="TasksAgent",
            llm_config=llm_config,
            system_message="Generate implementation task lists"
        )
        
        # Create design file for tasks generation
        design_path = Path(workspace) / "design.md"
        design_path.write_text(design_content)
        
        tasks_path = await self.execute_with_rate_limit_handling(
            lambda: tasks_agent.generate_task_list(str(design_path), requirements_path, workspace)
        )
        
        # Read the generated tasks content
        tasks_file_path = Path(tasks_path)
        assert tasks_file_path.exists(), f"Tasks file not created at {tasks_path}"
        tasks_content = tasks_file_path.read_text()
        
        return DocumentSet(
            requirements_content=requirements_content,
            design_content=design_content,
            tasks_content=tasks_content,
            user_request=user_request,
            project_name=Path(workspace).name
        )
    
    async def _generate_revised_documents(self, original_docs: DocumentSet, feedback: str, 
                                        managers, llm_config, memory_manager, workspace) -> DocumentSet:
        """
        Generate revised versions of documents based on feedback.
        
        Args:
            original_docs: Original document set
            feedback: Feedback for revision
            managers: Initialized managers
            llm_config: LLM configuration
            memory_manager: Memory manager instance
            workspace: Temporary workspace
            
        Returns:
            DocumentSet containing revised documents
        """
        # For simplicity, we'll use the _process_task_impl method with revision task type
        # Create design agent for revision using container
        from autogen_framework.dependency_container import DependencyContainer
        container = DependencyContainer.create_production(workspace, llm_config)
        
        design_agent = DesignAgent(
            container=container,
            name="DesignAgent",
            llm_config=llm_config,
            system_message="Generate technical design documents"
        )
        
        # Write original design to file for revision
        design_path = Path(workspace) / "design.md"
        design_path.write_text(original_docs.design_content)
        
        # Revise design using the agent's revision functionality
        design_revision_task = {
            "task_type": "revision",
            "revision_feedback": feedback,
            "work_directory": workspace,
            "current_result": {"design_path": str(design_path)}
        }
        
        revised_design_result = await self.execute_with_rate_limit_handling(
            lambda: design_agent._process_task_impl(design_revision_task)
        )
        
        assert revised_design_result['success'], f"Design revision failed: {revised_design_result.get('message')}"
        
        # Read revised design content
        revised_design_content = design_path.read_text()
        
        # Create tasks agent for revision
        tasks_agent = TasksAgent(
            llm_config=llm_config,
            token_manager=managers.token_manager,
            context_manager=managers.context_manager,
            memory_manager=memory_manager
        )
        
        # Write original tasks to file for revision
        tasks_path = Path(workspace) / "tasks.md"
        tasks_path.write_text(original_docs.tasks_content)
        
        # Revise tasks using the agent's revision functionality
        tasks_revision_task = {
            "task_type": "revision",
            "revision_feedback": feedback,
            "work_directory": workspace,
            "current_result": {"tasks_path": str(tasks_path)}
        }
        
        revised_tasks_result = await self.execute_with_rate_limit_handling(
            lambda: tasks_agent._process_task_impl(tasks_revision_task)
        )
        
        assert revised_tasks_result['success'], f"Tasks revision failed: {revised_tasks_result.get('message')}"
        
        # Read revised tasks content
        revised_tasks_content = tasks_path.read_text()
        
        return DocumentSet(
            requirements_content=original_docs.requirements_content,  # Requirements unchanged
            design_content=revised_design_content,
            tasks_content=revised_tasks_content,
            user_request=original_docs.user_request,
            project_name=original_docs.project_name
        )


# Test execution configuration
if __name__ == "__main__":
    pytest.main([__file__, "-v", "-s", "--tb=short"])